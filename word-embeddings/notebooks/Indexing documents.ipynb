{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing environmental documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will write a script that will index our documents. Each document will be given and ID. For each word that appears in the documents, we will mark the documents in which it appears.\n",
    "Suppose word 'car' appears in documents with IDs 3, 13 and 103. Then our index dictionary will contain a key 'car' with value [3,13,103]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 # Database acces\n",
    "import json # Output file\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user': 'postgres', 'dbname': 'eurlex_environment_only', 'port': '5432', 'tty': '', 'options': '', 'sslmode': 'prefer', 'sslcompression': '0', 'krbsrvname': 'postgres', 'target_session_attrs': 'any'}\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(user='postgres', password='dbpass', database='eurlex_environment_only')\n",
    "pg = conn.cursor()\n",
    "print(conn.get_dsn_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all documents from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.execute(\"\"\"\n",
    "SELECT * FROM documents\n",
    "\"\"\")\n",
    "all_documents = pg.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to prepare a mapping that will connect document ids to celex number and vice versa. Also we need to create an index dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2id = {} # Dictionary that will map from document CELEX number to ID\n",
    "id2doc = {} # Dictionary that will map from document ID to CELEX\n",
    "\n",
    "index = defaultdict(set) # Our index dictionary: keys (words), values (list of document ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function below we tranform document data into friendly structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def tokenize_document(doc):\n",
    "    \"\"\"\n",
    "    With this function we will tokenize the document. We will remove punctuation, put everything to\n",
    "    lower case, remove stopwords.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_id, celex, title, author, form, date, text = doc\n",
    "    lower = lambda x: x.lower()\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    for part in [title, author, form, text]:\n",
    "        if part is None: continue\n",
    "        part = preprocess_string(part, [lower, strip_punctuation])\n",
    "        words += [w for w in part if w not in stopwords]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start indexing. For each document we will index all the words that appear in the text and save them into index dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(all_documents):\n",
    "    \n",
    "    doc_id, celex = doc[0], doc[1]\n",
    "    doc2id[celex] = doc_id\n",
    "    id2doc[doc_id] = celex\n",
    "    \n",
    "    words = tokenize_document(doc)\n",
    "    for word in words:\n",
    "        index[word].add(doc_id)\n",
    "    \n",
    "    if i % 10000 == 0:\n",
    "        print(i, len(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.672238906345452\n",
      "122887\n"
     ]
    }
   ],
   "source": [
    "# Average size of index list\n",
    "print(sum(len(e) for e in index.values())/len(index))\n",
    "print(max(len(e) for e in index.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now save the ouput into index.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('index.json', 'w') as outfile:\n",
    "    json.dump({\n",
    "        'doc2id' : doc2id,\n",
    "        'id2doc' : id2doc,\n",
    "        'index' : {k : list(v) for k,v in index.items()}}, # We can't serialize set objects.\n",
    "        outfile, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{51225, 116762, 45085, 94239, 94240, 51234, 51241, 57386, 57387, 118827, 98351, 51249, 96316, 51270, 59481, 57435, 2141, 57437, 57447, 57448, 59499, 57453, 59503, 79995, 57469, 57470, 98437, 57480, 51343, 49311, 104609, 104610, 55460, 90277, 34997, 55485, 8399, 30930, 104659, 49372, 51423, 108771, 2276, 28912, 53493, 49399, 49405, 110847, 92431, 45329, 49426, 53523, 104721, 49447, 4399, 117039, 49458, 94514, 55606, 55607, 55613, 43338, 49486, 47439, 55639, 29018, 49499, 45406, 39267, 370, 371, 39295, 33164, 49557, 57752, 55714, 29099, 12716, 57773, 98732, 53680, 2484, 2485, 57781, 57782, 53692, 39358, 33219, 2500, 484, 49640, 49641, 41456, 31227, 41468, 16901, 53774, 43536, 53781, 43542, 53782, 53786, 33309, 53791, 53792, 53795, 57894, 57895, 55849, 53810, 115261, 53825, 53826, 57929, 51786, 53839, 57937, 4692, 43608, 94811, 49758, 49765, 57957, 57962, 57965, 55928, 57979, 17032, 57994, 98967, 12956, 55967, 101026, 53931, 78513, 53938, 53949, 53950, 53951, 51906, 51907, 94917, 45770, 51915, 105164, 53973, 53974, 43739, 45792, 45795, 45803, 54005, 54006, 54007, 54008, 2809, 54017, 54020, 54021, 54025, 51980, 51983, 51989, 51992, 109347, 101158, 74546, 52041, 97110, 43881, 47979, 52076, 35696, 52081, 47989, 52090, 47995, 52096, 50052, 50053, 52105, 103305, 50062, 924, 52127, 52128, 52130, 50084, 52133, 48038, 52140, 52141, 52144, 95155, 46004, 117687, 58308, 58314, 48082, 58323, 58324, 58325, 58327, 58328, 56281, 56282, 58329, 58330, 58331, 58332, 58333, 58336, 58334, 58335, 58339, 58337, 58341, 58338, 58340, 58343, 58344, 58345, 58346, 58347, 58348, 58349, 58350, 103425, 99334, 101383, 103431, 56350, 56351, 44068, 95280, 99386, 50243, 52307, 52309, 52310, 50271, 56433, 56435, 93299, 48252, 50300, 56449, 56462, 105614, 56464, 50321, 91288, 50332, 91293, 56478, 48286, 50346, 97451, 50349, 50352, 50355, 50359, 46266, 46273, 50369, 50389, 50391, 111832, 50393, 46301, 46304, 97511, 50408, 54508, 101628, 54528, 56586, 27917, 58638, 56601, 1310, 113953, 30009, 56633, 56634, 97597, 56644, 58714, 50527, 58741, 58746, 32123, 71034, 54654, 58756, 52616, 58772, 48535, 28058, 38310, 50599, 103856, 54716, 54718, 54723, 54726, 50636, 52685, 54741, 81392, 34298, 48647, 105991, 105992, 38417, 44566, 48669, 48675, 93731, 93732, 93734, 93733, 93735, 48687, 52791, 52795, 52796, 48708, 48721, 52824, 48730, 116321, 52837, 108133, 99944, 54899, 1659, 56962, 1673, 56970, 48780, 13965, 52881, 54940, 54941, 108188, 54943, 56991, 52897, 52901, 54949, 52911, 44725, 52920, 52921, 52924, 114376, 89801, 44756, 55000, 52959, 55007, 59103, 57072, 57073, 52984, 59133, 59134, 50943, 50951, 50970, 50982, 46889, 98091, 57132, 48941, 57140, 59189, 57144, 59194, 59195, 48957, 59197, 48965, 46918, 48966, 59206, 100165, 100166, 112456, 48978, 46931, 53079, 48984, 116571, 57182, 46943, 48994, 59240, 59244, 53101, 51057, 1906, 53107, 98169, 59258, 57217, 57218, 57219, 53124, 108419, 53127, 51089, 57237, 38806, 59291, 47007, 94113, 100265, 98220, 51117, 71597, 96188, 2004, 57301, 51161, 110555, 51167, 57315, 51180, 30702, 51185, 59379, 110584, 77817, 34811}\n"
     ]
    }
   ],
   "source": [
    "print(index['deforestation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
