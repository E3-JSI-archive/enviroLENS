{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing environmental documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will write a script that will index our documents. Each document will be given and ID. For each word that appears in the documents, we will mark the documents in which it appears.\n",
    "Suppose word 'car' appears in documents with IDs 3, 13 and 103. Then our index dictionary will contain a key 'car' with value [3,13,103]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 # Database acces\n",
    "import json # Output file\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user': 'postgres', 'dbname': 'eurlex_environment_only', 'port': '5432', 'tty': '', 'options': '', 'sslmode': 'prefer', 'sslcompression': '0', 'krbsrvname': 'postgres', 'target_session_attrs': 'any'}\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(user='postgres', password='dbpass', database='eurlex_environment_only')\n",
    "pg = conn.cursor()\n",
    "print(conn.get_dsn_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all documents from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.execute(\"\"\"\n",
    "SELECT * FROM documents\n",
    "\"\"\")\n",
    "all_documents = pg.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to prepare a mapping that will connect document ids to celex number and vice versa. Also we need to create an index dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2id = {} # Dictionary that will map from document CELEX number to ID\n",
    "id2doc = {} # Dictionary that will map from document ID to CELEX\n",
    "\n",
    "index = defaultdict(set) # Our index dictionary: keys (words), values (list of document ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function below we tranform document data into friendly structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def tokenize_document(doc):\n",
    "    \"\"\"\n",
    "    With this function we will tokenize the document. We will remove punctuation, put everything to\n",
    "    lower case, remove stopwords.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_id, celex, title, author, form, date, text = doc\n",
    "    lower = lambda x: x.lower()\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    for part in [title, author, form, text]:\n",
    "        if part is None: continue\n",
    "        part = preprocess_string(part, [lower, strip_punctuation])\n",
    "        words += [w for w in part if w not in stopwords]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start indexing. For each document we will index all the words that appear in the text and save them into index dictionary. And we will also add the document id to the word set in case that word appears inside this documents text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 85\n",
      "10000 286227\n",
      "20000 418779\n",
      "30000 792069\n",
      "40000 1219448\n",
      "50000 1527521\n",
      "60000 1849271\n",
      "70000 1871015\n",
      "80000 2018149\n",
      "90000 2019121\n",
      "100000 2019645\n",
      "110000 2020269\n",
      "120000 2026726\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(all_documents):\n",
    "    \n",
    "    doc_id, celex = doc[0], doc[1]\n",
    "    doc2id[celex] = doc_id\n",
    "    id2doc[doc_id] = celex\n",
    "    \n",
    "    words = tokenize_document(doc)\n",
    "    for word in words:\n",
    "        index[word].add(doc_id)\n",
    "    \n",
    "    if i % 10000 == 0:\n",
    "        print(i, len(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same procedure for document descriptors. We will make a small adaptation there and also add full document descriptors in our indexing vocabulary. For example descriptor 'Air pollution' will add words 'air', 'pollution' and 'air pollution' to the index vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descriptors = pg.execute(\n",
    "    \"\"\"SELECT * FROM document_descriptors\"\"\"\n",
    ")\n",
    "all_descriptors = pg.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12001C_DCL_09\n",
      "100000 32002D0995\n",
      "200000 32014R1211\n",
      "300000 52007SC0274\n",
      "400000 62014CA0167\n",
      "500000 91985E002108\n",
      "600000 91993E000577\n",
      "700000 92002E003658\n",
      "800000 92013E005407\n"
     ]
    }
   ],
   "source": [
    "for i, (celex, descriptor) in enumerate(all_descriptors):\n",
    "    \n",
    "    doc_id = doc2id[celex]\n",
    "    \n",
    "    # Remove punctuation and make the string lowercase\n",
    "    descriptor = strip_punctuation(descriptor).lower()\n",
    "    for word in descriptor.split(' '):\n",
    "        if word not in stopwords:\n",
    "            index[word].add(doc_id)\n",
    "    \n",
    "    index[descriptor].add(doc_id) # we include the whole descriptor in our vocabulary\n",
    "    \n",
    "    if i % 100000 == 0:\n",
    "        print(i, celex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we also repeat similar procedure for document subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjects = pg.execute(\n",
    "    \"\"\"SELECT * FROM document_subjects\"\"\"\n",
    ")\n",
    "all_subjects = pg.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12001C_DCL_09\n",
      "100000 92011E002990\n"
     ]
    }
   ],
   "source": [
    "for i, (celex, subject) in enumerate(all_subjects):\n",
    "    \n",
    "    doc_id = doc2id[celex]\n",
    "    \n",
    "    # Remove punctuation and make the string lower\n",
    "    subject = strip_punctuation(subject).lower()\n",
    "    for word in subject.split(' '):\n",
    "        if word not in stopwords:\n",
    "            index[word].add(doc_id)\n",
    "    \n",
    "    index[subject].add(doc_id) # we include the whole descriptor in our vocabulary\n",
    "    \n",
    "    if i % 100000 == 0:\n",
    "        print(i, celex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.494677999018144\n",
      "122911\n"
     ]
    }
   ],
   "source": [
    "# Average size of index list\n",
    "print(sum(len(e) for e in index.values())/len(index))\n",
    "print(max(len(e) for e in index.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now save the ouput into index.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_reformated = {k : sorted(list(v)) for k,v in index.items()} # We can't serialize set objects.\n",
    "\n",
    "with open('index.json', 'w') as outfile:\n",
    "    json.dump({\n",
    "        'doc2id' : doc2id,\n",
    "        'id2doc' : id2doc,\n",
    "        'index' : index_reformated},\n",
    "        outfile, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[370, 371, 484, 924, 1310, 1659, 1673, 1906, 2004, 2141, 2276, 2484, 2485, 2500, 2809, 4399, 4692, 8399, 12716, 12956, 13965, 16901, 17032, 27917, 28058, 28912, 29018, 29099, 30009, 30702, 30930, 31227, 32123, 33164, 33219, 33309, 34298, 34811, 34997, 35696, 38310, 38417, 38806, 39267, 39295, 39358, 40195, 40564, 40601, 41456, 41468, 41508, 41522, 41804, 41805, 41806, 41866, 42109, 42110, 42114, 42125, 42162, 42409, 42421, 42653, 42688, 42718, 42867, 42947, 42959, 43238, 43338, 43536, 43542, 43608, 43739, 43881, 44068, 44566, 44725, 44756, 45085, 45329, 45406, 45770, 45792, 45795, 45803, 46004, 46266, 46273, 46301, 46304, 46889, 46918, 46931, 46943, 47007, 47439, 47979, 47989, 47995, 48038, 48082, 48252, 48286, 48535, 48647, 48669, 48675, 48687, 48708, 48721, 48730, 48780, 48941, 48957, 48965, 48966, 48978, 48984, 48994, 49311, 49372, 49399, 49405, 49426, 49447, 49458, 49486, 49499, 49557, 49640, 49641, 49758, 49765, 50052, 50053, 50062, 50084, 50243, 50271, 50300, 50321, 50332, 50346, 50349, 50352, 50355, 50359, 50369, 50389, 50391, 50393, 50408, 50527, 50599, 50636, 50943, 50951, 50970, 50982, 51057, 51089, 51117, 51161, 51167, 51180, 51185, 51225, 51234, 51241, 51249, 51270, 51343, 51423, 51786, 51906, 51907, 51915, 51980, 51983, 51989, 51992, 52041, 52076, 52081, 52090, 52096, 52105, 52127, 52128, 52130, 52133, 52140, 52141, 52144, 52307, 52309, 52310, 52616, 52685, 52791, 52795, 52796, 52810, 52824, 52837, 52881, 52897, 52901, 52911, 52920, 52921, 52924, 52959, 52984, 53079, 53101, 53105, 53107, 53124, 53127, 53493, 53523, 53680, 53692, 53774, 53781, 53782, 53786, 53791, 53792, 53795, 53810, 53825, 53826, 53839, 53931, 53938, 53949, 53950, 53951, 53973, 53974, 54005, 54006, 54007, 54008, 54017, 54020, 54021, 54025, 54508, 54528, 54654, 54716, 54718, 54723, 54726, 54741, 54769, 54899, 54940, 54941, 54943, 54949, 55000, 55007, 55460, 55485, 55606, 55607, 55613, 55639, 55714, 55849, 55928, 55967, 56140, 56281, 56282, 56350, 56351, 56433, 56435, 56449, 56462, 56464, 56478, 56586, 56601, 56633, 56634, 56644, 56962, 56970, 56991, 57072, 57073, 57132, 57140, 57144, 57182, 57217, 57218, 57219, 57237, 57301, 57315, 57386, 57387, 57435, 57437, 57447, 57448, 57453, 57469, 57470, 57480, 57752, 57773, 57781, 57782, 57894, 57895, 57929, 57937, 57957, 57962, 57965, 57979, 57994, 58308, 58314, 58323, 58324, 58325, 58327, 58328, 58329, 58330, 58331, 58332, 58333, 58334, 58335, 58336, 58337, 58338, 58339, 58340, 58341, 58343, 58344, 58345, 58346, 58347, 58348, 58349, 58350, 58638, 58714, 58741, 58746, 58756, 58772, 59103, 59133, 59134, 59189, 59194, 59195, 59197, 59206, 59240, 59244, 59258, 59291, 59379, 59481, 59499, 59503, 64680, 68285, 68306, 68960, 69199, 69519, 69917, 70585, 70607, 70681, 70990, 71034, 71183, 71471, 71597, 71693, 71875, 71895, 71896, 72238, 72672, 72673, 72820, 73057, 73072, 73201, 73667, 74546, 74617, 75600, 75889, 76296, 77817, 77912, 77913, 78138, 78499, 78513, 78622, 78812, 79401, 79718, 79753, 79754, 79765, 79991, 79995, 80125, 80363, 81127, 81392, 81435, 81990, 82031, 82256, 82275, 82289, 82292, 82293, 82306, 82414, 82415, 82416, 82513, 82590, 82689, 82840, 82876, 83212, 83243, 83244, 83247, 83253, 83255, 83256, 83461, 83501, 83572, 83633, 83951, 83952, 83953, 83954, 83955, 83956, 83957, 83958, 83959, 83960, 83961, 83962, 83963, 83964, 83965, 83966, 83967, 84024, 84096, 84406, 84604, 84764, 84794, 84795, 84822, 84989, 85602, 85742, 86439, 87518, 87528, 87760, 88948, 89260, 89742, 89801, 89852, 90277, 90548, 91288, 91293, 92032, 92431, 93238, 93299, 93731, 93732, 93733, 93734, 93735, 94113, 94157, 94239, 94240, 94263, 94514, 94710, 94811, 94917, 95155, 95280, 96188, 96316, 96390, 96952, 97110, 97451, 97511, 97597, 97996, 98091, 98169, 98220, 98351, 98437, 98732, 98801, 98802, 98910, 98967, 99209, 99334, 99386, 99519, 99538, 99944, 100165, 100166, 100199, 100265, 100795, 101026, 101158, 101383, 101628, 101703, 101872, 102055, 102455, 102536, 102912, 102913, 102918, 103305, 103425, 103431, 103856, 103921, 104514, 104516, 104609, 104610, 104659, 104721, 105130, 105164, 105175, 105250, 105424, 105425, 105592, 105614, 105830, 105949, 105991, 105992, 106208, 106448, 106544, 106554, 106697, 106846, 106936, 107089, 107293, 107308, 107314, 107986, 108133, 108135, 108188, 108419, 108771, 108801, 108971, 109165, 109347, 109378, 110223, 110224, 110333, 110335, 110363, 110384, 110434, 110555, 110584, 110728, 110847, 111049, 111784, 111832, 111885, 112312, 112456, 112721, 113951, 113953, 114331, 114376, 114545, 114562, 115261, 115783, 115927, 116009, 116321, 116571, 116624, 116762, 117032, 117039, 117453, 117629, 117687, 118827]\n"
     ]
    }
   ],
   "source": [
    "print(index_reformated['deforestation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
